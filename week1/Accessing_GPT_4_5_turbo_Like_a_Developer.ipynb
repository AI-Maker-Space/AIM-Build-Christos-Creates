{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.5-turbo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4SKfBCSB8ds"
      },
      "outputs": [],
      "source": [
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "748eabc5-e131-43ab-bc90-3720e81e5baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/quickstart/step-3-sending-your-first-api-request) if you get stuck!\n",
        "\n",
        "Let's create a `chat.completions` instance to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "9a3f12de-ec88-4e90-9d93-d389669b8273"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-8No0SEJVrtk52Vl3ZfAZd9g2Ve9zN', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two tools related to language models and AI, developed for different purposes.\\n\\nAs of my last update, LangChain is an open-source library and framework designed to make it easier to build applications with LLMs (large language models). LangChain is created by Language.ai and offers abstractions and APIs to help developers integrate language AI into their apps without the complexities of managing the underlying model interactions directly. The goal of LangChain is to facilitate the development of language-driven workflows and applications by providing a set of out-of-the-box components for common tasks that you might want to perform with a language model, such as question answering or summarization.\\n\\nOn the other hand, LlamaIndex, as far as I am aware, refers to the Llama Index created by OpenAI, which is a tool developed to keep track of the impacts Large Language Models (LLMs) (like GPT-3) have on energy consumption and compute used. The index aims to help researchers and practitioners become more conscious about the environmental impacts of using such models and drive the community towards more efficient and sustainable practices by providing a carbon and compute impact estimator for work involving large language models.\\n\\nWhile both terms include the \"Llama\"/\"Llm\" track, they refer to different initiatives within the field of AI and have different goals. LangChain is focused on application development and abstraction over language models, while LlamaIndex is more about the environmental impact and sustainability of using such models.', role='assistant', function_call=None, tool_calls=None))], created=1700684460, model='gpt-4-1106-preview', object='chat.completion', system_fingerprint='fp_a24b4d720c', usage=CompletionUsage(completion_tokens=298, prompt_tokens=19, total_tokens=317))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\", # or gpt-3.5-turbo\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(messages: str, model: str = \"gpt-4-1106-preview\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "b96610c6-d6a8-4f5a-aa49-630ee9980c98"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As of my knowledge cutoff date in 2023, LangChain and LlamaIndex are not established terms in widespread use across the technology or linguistic fields. It is possible they could be specific tools, libraries, or services introduced after my last update, or proprietary technologies not publicly documented. Without additional context or up-to-date information, it is difficult to provide an accurate comparison.\n",
              "\n",
              "To give you the best possible information, please provide more details about LangChain and LlamaIndex, such as their application areas, industries they are associated with, or links to relevant resources or documentation. With more context, I could either supply an informative comparison or direct you to the appropriate resources.\n",
              "\n",
              "If these terms relate to a hypothetical scenario, emerging tech products, or niche applications, specifying their intended functions or the context in which you came across them could be beneficial for providing an accurate explanation.\n",
              "\n",
              "If they appeared after my last update and are gaining traction or use in specific fields, I would recommend visiting the official websites or resources associated with LangChain and LlamaIndex for the most accurate and up-to-date information."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(get_response([user_prompt(YOUR_PROMPT)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `system` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The system message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the system prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "e8f276e0-2fb2-4764-faa2-45c8a214b37b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As an AI, I don't have personal preferences or the ability to consume food or beverages. However, I can tell you that the choice between crushed ice and cubed ice often depends on individual preference and the specific use case:\n",
              "\n",
              "1. **Crushed Ice**: \n",
              "   - Melts faster due to its increased surface area, which can quickly chill a drink.\n",
              "   - Often used in cocktails, smoothies, and slushies because it blends well and creates a smoother texture.\n",
              "   - Can water down drinks more quickly because it melts faster.\n",
              "\n",
              "2. **Cubed Ice**:\n",
              "   - Melts slower, keeping drinks cool for a longer time without diluting them as quickly.\n",
              "   - Typically used for serving sodas, water, and spirits on the rocks.\n",
              "   - Larger cubes, such as those from a sphere or block mold, can melt even slower and are often preferred for high-quality spirits.\n",
              "\n",
              "For someone who is extremely thirsty, crushed ice might provide immediate refreshment, while cubed ice could be better for sipping a drink over a longer period without too much dilution. If you have preferences or specific needs for your drink, those will dictate which form of ice is better for you."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "0460ae89-e7e1-4bb0-d1ce-37632238d3bd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As an AI, I don't have preferences or the ability to consume beverages, so I don't have a preference for crushed ice or cubed ice. However, both types serve different purposes and can enhance the drinking experience in different ways.\n",
              "\n",
              "Crushed ice tends to make drinks colder faster because it has a larger surface area that comes in contact with the beverage. It also tends to melt faster, which can be good for quickly chilling a drink but might dilute it more quickly. Crushed ice is often preferred for slushy drinks, tiki or tropical cocktails, and other beverages where a quick chill and a certain textural experience are desired.\n",
              "\n",
              "Cubed ice, on the other hand, melts more slowly due to the smaller surface area in contact with the liquid. This makes it ideal for drinks that you want to keep cold for a longer period without excessive dilution, such as whiskey on the rocks or a classic cocktail like an Old Fashioned.\n",
              "\n",
              "Ultimately, the choice between crushed ice and cubed ice comes down to personal preference and the specific application in the context of beverages."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "898220eb-26f1-47fd-e9d0-8401970cc69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-8No1AFTEDexUV7rEZhgGKI1gO9qUE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"As an AI, I don't have preferences or the ability to consume beverages, so I don't have a preference for crushed ice or cubed ice. However, both types serve different purposes and can enhance the drinking experience in different ways.\\n\\nCrushed ice tends to make drinks colder faster because it has a larger surface area that comes in contact with the beverage. It also tends to melt faster, which can be good for quickly chilling a drink but might dilute it more quickly. Crushed ice is often preferred for slushy drinks, tiki or tropical cocktails, and other beverages where a quick chill and a certain textural experience are desired.\\n\\nCubed ice, on the other hand, melts more slowly due to the smaller surface area in contact with the liquid. This makes it ideal for drinks that you want to keep cold for a longer period without excessive dilution, such as whiskey on the rocks or a classic cocktail like an Old Fashioned.\\n\\nUltimately, the choice between crushed ice and cubed ice comes down to personal preference and the specific application in the context of beverages.\", role='assistant', function_call=None, tool_calls=None))], created=1700684504, model='gpt-4-1106-preview', object='chat.completion', system_fingerprint='fp_a24b4d720c', usage=CompletionUsage(completion_tokens=218, prompt_tokens=30, total_tokens=248))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bc60f158-98da-41ac-f7db-466fc7922b3a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I apologize for any confusion, but 'stimple' and 'falbean' do not appear to be recognized words in the English language. They may be non-standard, made-up, or extremely obscure words, or simply a typographical error. If you meant these to be fictional or playful terms, I can certainly create a sentence incorporating them:\n",
              "\n",
              "\"While wandering through the whimsical garden, I stumbled upon a curious plant called a 'falbean', which bore fruit that tasted like a fusion of fennel and beans, and nearby was the twisting 'stimple' vine known for its ability to dance in the softest of breezes.\"\n",
              "\n",
              "If these words have specific meanings in a particular context or if you would like me to use different words, please let me know!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "4InUN_ArZJpa",
        "outputId": "9c43bbce-839d-4b58-e90d-9c2e59587d95"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As a reminder, 'stimple' and 'falbean' are not established words in the English language, so I will craft a hypothetical sentence assuming these terms have the meanings you've given:\n",
              "\n",
              "\"The mechanic admired the new device, noting, 'This stimple falbean makes the assembly process much more efficient, as it spins smoothly and tightens bolts reliably without any fuss.'\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "2ed98a7f-3452-4b03-b866-2dd290734d87"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Calculating the time needed for each option and considering the time zone differences is necessary to determine which travel option will allow Billy to get home by 7 PM EDT.\n",
              "\n",
              "First, it's important to note the time zone difference between San Francisco and Eastern Daylight Time (EDT). San Francisco is in the Pacific Time Zone, which is 3 hours behind EDT.\n",
              "\n",
              "Option 1: Fly (3hrs) + Bus (2hrs)\n",
              "If it's currently 1 PM in San Francisco (PDT), adding the flight and bus travel time gives a total of 5 hours:\n",
              "1 PM + 5 hours = 6 PM PDT\n",
              "Since EDT is 3 hours ahead, 6 PM PDT would be 9 PM EDT. Thus, Billy would not make it home by 7 PM EDT using the fly+bus option.\n",
              "\n",
              "Option 2: Teleporter (0hrs) + Bus (1hr)\n",
              "If Billy takes the teleporter and the bus, the total travel time would be:\n",
              "1 PM + 1 hour = 2 PM PDT\n",
              "Since EDT is 3 hours ahead, 2 PM PDT would be 5 PM EDT. With this option, Billy would make it home by 7 PM EDT.\n",
              "\n",
              "Therefore, it does matter which option Billy selects. To get home before 7 PM EDT, Billy should take the teleporter and then the bus, which will get him home by 5 PM EDT. The flying option would get him home too late."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "HpKEt7Z5fo4s",
        "outputId": "64493749-8c12-43e2-c93e-f22e95d984aa"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "First, let's clarify the time difference between San Francisco (Pacific Time Zone) and Eastern Time Zone. The Eastern Time Zone is typically 3 hours ahead of Pacific Time.\n",
              "\n",
              "It's currently 1PM in San Francisco, which means it's currently 4PM EDT.\n",
              "\n",
              "Billy wants to get home before 7PM EDT. That gives Billy 3 hours to make the journey from the current time of 4PM EDT.\n",
              "\n",
              "Option 1: Fly (3hrs) and take a bus (2hrs)\n",
              "\n",
              "- If Billy chooses to fly, the flight will take 3 hours. If Billy departs at 1PM local time (4PM EDT), the flight will arrive at 4PM local time (7PM EDT).\n",
              "- After the flight, taking the bus will add another 2 hours to the journey, meaning Billy wouldn't arrive at the final destination until 6PM local time, which translates to 9PM EDT.\n",
              "\n",
              "Option 2: Teleporter (0hrs) and take a bus (1hr)\n",
              "\n",
              "- If Billy uses the teleporter, there is no travel time for the teleporter itself, so Billy would be ready to take the bus immediately.\n",
              "- The bus would take 1 hour to reach the final destination. So if Billy teleports at 1PM local time (4PM EDT), the bus journey would be finished by 2PM local time, which is 5PM EDT.\n",
              "\n",
              "So by taking the second option (teleporter and bus), Billy would arrive home well before the 7PM EDT deadline. The first option of flying and taking the bus would result in Billy arriving after the deadline.\n",
              "\n",
              "Therefore, it does matter which travel option Billy selects. To get home before 7PM EDT, Billy must choose the teleporter and then take the bus."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHH8zof-gkc1"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing OpenAI's `gpt-4-1106-preview` through an API, developer style, you can start building simple applications built on top.\n",
        "\n",
        "Check out creating a simple application powered by `gpt-3.5-turbo`!\n",
        "You can use the [Beyond-ChatGPT repo](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) steps as a template!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)\n",
        "<br>\n",
        "Updated for the [openai api migration to v1.0.0](https://github.com/openai/openai-python/discussions/742) by [Christos Magganas](https://www.linkedin.com/in/christos-magganas/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
